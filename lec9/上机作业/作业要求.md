# 岭回归 Ridge Regression

岭回归是普通线性回归的 \( L_2 \) 正则版本。

原始线性回归的损失函数为：

\[Loss = \|y - X\beta\|_2^2\]

\[\min_{\beta} \|y - X\beta\|_2^2\]

其解析解为：

\[\hat{\beta} = (X^T X)^{-1} X^T y\]

岭回归在损失函数中增加了一个权重二范数损失项的平方（\(\lambda > 0\)）

\[Loss = \|y - X\beta\|_2^2 + \lambda \|\beta\|_2^2\]

\[\min_{\beta} \|y - X\beta\|_2^2 + \lambda \|\beta\|_2^2\]

其解析解为：

\[\hat{\beta} = (X^T X + \lambda I)^{-1} X^T y\]

# Lasso回归

岭回归的损失函数“告诉”模型：不要将特征的值学习的很大。但是特征虽小，仍不为0。对于噪声数据而言，岭回归几乎总会学习为全部特征都非零。然而真实模型却不是这样的。

为了达到稀疏回归的目的。自动选择重要特征、去掉不相关的特征，得到更简洁、可解释的模型。Lasso回归方法被提出。

**Lasso的核心思想：**

告诉模型：可以拟合数据，但请尽量用少的特征（不需要的特征就保持为0）。

方法：在损失函数中加入对系数绝对值的惩罚。

\[ Loss = \|y - X\beta\|_2^2 + \lambda\|\beta\|_1 \]

使用 **lasso_reg.m** 对比Lasso和另外两种回归的区别。


# 实验要求
使用MATLAB完成以下任务：

1. 数据加载与预处理：
   • 使用附件中的diabetes.csv数据集
   • 将数据分为训练集（70%）和测试集（30%）
   • 对特征进行标准化处理

2. 模型训练与比较：
   • 实现普通线性回归
   • 实现岭回归，在λ=[0.001, 0.01, 0.1, 1, 10, 100]中寻找最优参数
   • 实现Lasso回归，在相同范围内寻找最优参数

3. 结果分析：
   • 比较三种方法（普通线性回归、岭回归、Lasso回归）在测试集上的MSE
   • 分析各方法选择的特征数量（稀疏性）
   • 绘制系数路径图
   • 讨论哪种方法最适合这个数据集并说明原因